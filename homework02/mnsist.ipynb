{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How many training/test images are there?\n",
    "    - 60,000 train images\n",
    "    - 10,000 test images\n",
    "\n",
    "### What's the image shape?\n",
    "    - shape = (28, 28, 1)\n",
    "\n",
    "### What range are pixel values in?\n",
    "    - the range goes from 0 to 255 (grayscale values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_pipeline(train_ds, test_ds, batch_size):\n",
    "    train_ds = train_ds.map(lambda feature_dict: (feature_dict['image'], feature_dict['label'])) # extracting image and labels\n",
    "    train_ds = train_ds.map(lambda image, label: (tf.reshape(image,(-1,)), label)) # reshape from, 28,28,1 to one vector\n",
    "    train_ds = train_ds.map(lambda image, label: ((tf.cast(image, tf.float32)/128)-1, label)) # rescaling the values\n",
    "    train_ds = train_ds.map(lambda image, label: (image, tf.one_hot(label, depth=10))) # one-hot encoder\n",
    "    train_ds = train_ds.shuffle(1024).batch(4) # taking batches of 4 out of 1024\n",
    "    train_ds = train_ds.prefetch(4) # always having 4 minibatches ready for the gpu (to minimize the runtime); splitting work between cpu and gpu\n",
    "\n",
    "    test_ds = test_ds.map(lambda feature_dict: (feature_dict['image'], feature_dict['label'])) # extracting image and labels\n",
    "    test_ds = test_ds.map(lambda image, label: (tf.reshape(image,(-1,)), label)) # reshape from, 28,28,1 to one vector\n",
    "    test_ds = test_ds.map(lambda image, label: ((tf.cast(image, tf.float32)/128)-1, label)) # rescaling the values\n",
    "    test_ds = test_ds.map(lambda image, label: (image, tf.one_hot(label, depth=10))) # one-hot encoder\n",
    "    test_ds = test_ds.shuffle(1024).batch(batch_size) # taking batches of 4 out of 1024\n",
    "    test_ds = test_ds.prefetch(4) # always having 4 minibatches ready for the gpu (to minimize the runtime); splitting work between cpu and gpu\n",
    "    return train_ds, test_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model Creation via Subclassing from tf.keras.Model\n",
    "class MLP_Model(tf.keras.Model):\n",
    "  def __init__(self, layer_sizes, output_size = 10): # the output size should be 10, because we are dealing with 10 different digits\n",
    "    super().__init__()\n",
    "    self.mlp_layers = []\n",
    "    #layer_sizes e.g. [256, 256]\n",
    "    for layer_size in layer_sizes:\n",
    "      new_layer = tf.keras.layers.Dense(units = layer_size, activation = 'sigmoid') # whole implementation for one layer\n",
    "      self.mlp_layers.append(new_layer)\n",
    "    self.output_layer = tf.keras.layers.Dense( units = output_size, activation = 'softmax')\n",
    "\n",
    "  def call(self, x):\n",
    "    for layer in self.mlp_layers:\n",
    "      x = layer(x)\n",
    "    y = self.output_layer(x)\n",
    "    return y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(n_epochs, model, ds_train, ds_test, loss_function, optimizer):\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    test_accuracies = []\n",
    "    for epoch in range(n_epochs):\n",
    "        epoch_losses = []\n",
    "        for x, target in ds_train:\n",
    "            with tf.GradientTape() as tape:\n",
    "                pred = model(x)\n",
    "                loss = loss_function(target, pred)\n",
    "            gradients = tape.gradient(loss, model.variables) # we want to calculate the gradient outside the gradient tape\n",
    "            optimizer.apply_gradients(zip(gradients, model.variables))\n",
    "            epoch_losses.append(loss.numpy())\n",
    "        train_losses.append(tf.reduce_mean(epoch_losses))\n",
    "\n",
    "        test_accuracy_aggregator = []\n",
    "        test_loss_aggregator = []\n",
    "\n",
    "        for x, target in ds_test:\n",
    "            prediction = model(x)\n",
    "            sample_test_loss = loss_function(target, prediction)\n",
    "            sample_test_accuracy =  np.argmax(target, axis=1) == np.argmax(prediction, axis=1)\n",
    "            sample_test_accuracy = np.mean(sample_test_accuracy)\n",
    "            test_loss_aggregator.append(sample_test_loss.numpy())\n",
    "            test_accuracy_aggregator.append(np.mean(sample_test_accuracy))\n",
    "\n",
    "        test_losses.append(tf.reduce_mean(test_loss_aggregator))\n",
    "        test_accuracies.append(tf.reduce_mean(test_accuracy_aggregator))\n",
    "\n",
    "    return train_losses, test_losses, test_accuracies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(n_epochs = 10, lay_sizes = [256,256], learn_rate = 0.01, batch_size = 4):\n",
    "  #load the data\n",
    "  train_ds = tfds.load('mnist', split = 'train')\n",
    "  test_ds = tfds.load('mnist', split = 'test')\n",
    "  \n",
    "  # initialize the model, the loss function and the optimizer\n",
    "  model = MLP_Model(layer_sizes = lay_sizes)\n",
    "  cce = tf.keras.losses.CategoricalCrossentropy()\n",
    "  optimizer = tf.keras.optimizers.legacy.SGD(learning_rate = learn_rate)\n",
    "\n",
    "  # orginize your data\n",
    "  train_dataset, test_dataset = data_pipeline(train_ds, test_ds, batch_size)\n",
    "  #just for showcasing I use .take, but generally you should train your model on the whole data\n",
    "  train_dataset = train_dataset.take(1000)\n",
    "  test_dataset = test_dataset.take(100)\n",
    "\n",
    "  # Initialize lists for later visualization.\n",
    "  train_losses = []\n",
    "  test_losses = []\n",
    "  test_accuracies = []\n",
    "\n",
    "  #the training loop\n",
    "  train_losses, test_losses, test_accuracies = training_loop(n_epochs, model, train_dataset, test_dataset, cce, optimizer)\n",
    "\n",
    "  # visualize the data\n",
    "  plt.figure()\n",
    "  line1, = plt.plot(train_losses)\n",
    "  line2, = plt.plot(test_losses)\n",
    "  line3, = plt.plot(test_accuracies)\n",
    "  plt.xlabel(\"Training steps\")\n",
    "  plt.ylabel(\"Loss/Accuracy\")\n",
    "  plt.legend((line1,line2, line3),(\"training\",\"test\", \"test accuracy\"))\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the initial model\n",
    "main()\n",
    "\n",
    "# high lerning_rate\n",
    "main(learn_rate = 1)\n",
    "\n",
    "# low layer_size\n",
    "main(lay_sizes = [32,32])\n",
    "\n",
    "# low batch_size\n",
    "main(batch_size = 1)\n",
    "\n",
    "# smaller number ov epochs\n",
    "main(n_epochs = 3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 Adjusting the hyperparameters of your models\n",
    "  - 1. If the learning rate is too high, the model will not be able to assign the images correctly to the pictures. Problems such as 'overshooting' could occur.\n",
    "  - 2. Even a model with two layers, each of which only consists of 32 units, produces somewhat decent results.\n",
    "  - 3. With a batch_size of 1, you can see in the plot that the variables fluctuate, which is one of the reasons why you should choose a larger batch size.\n",
    "  - 4. If the number of epochs is too low, the model does not receive enough training data, which is why the losses are still relatively high."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensor",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
